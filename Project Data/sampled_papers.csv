article,abstract
"moving mesh methods have been employed widely to approximate solutions of partial differential equations which exhibit large solution variations , such as shock waves and boundary or interior layers .several moving mesh approaches have been derived and many authors have discussed the significant improvements in accuracy and efficiency that can be achieved with respect to fixed mesh methods .  the moving mesh pde ( or mmpde ) approach has proven particularly effective in solving nonlinear pdes that exhibit solutions having some type of singularity , such as self - similar blow - up  or moving fronts  . for blow - up problems in particular , movingmesh methods permit a detailed study of the singularity formation with a degree of accuracy and efficiency that is simply not possible using fixed mesh methods .the primary advantage of the moving mesh approach stems from its ability to exploit special features of the solution ( such as self - similarity ) and build them directly into the numerical scheme .  in the mmpde approach ,a separate pde is derived to evolve the mesh points in such a way that they tend towards an equidistributed mesh at steady state , in the sense that the mesh points are positioned in space so as to equally distribute some measure of the solution error .the mmpde is coupled nonlinearly to the physical pde of interest , and both pdes are solved simultaneously .a key parameter in the moving mesh equation is the _ mesh relaxation time , _ usually denoted as ; the exact equidistribution equation is notoriously ill - conditioned  and so acts to regularize the mesh evolution in time . the philosophy behind introducing temporal smoothing , instead of equidistributing exactly ,is that the mesh need not be solved to the same level of accuracy as the physical pde ; in fact , solution accuracy can still be significantly improved over fixed mesh methods by only approximately equidistributing the mesh .  in previous results reported in the literature , the mesh relaxation time is invariably taken to be a constant for any given simulation .furthermore , huang , ren and russell observed in that _ `` while the parameter is critical , in our experience the numerical methods are relatively insensitive to the actual choice of in applications , '' _ and similar comments were made in .however , it is essential to keep in mind that these observations were made for problems in which the range of time scales present in the solution was fairly limited . in practice, must be tuned manually to optimize the behaviour of the computed mesh , and sometimes even to obtain a convergent numerical solution .the main purpose of this paper is to consider situations where taking constant may not be appropriate . keeping in mind that can be interpreted as a time scale for the mesh motion ,then should in fact be taken as a solution - dependent parameter , because as singularities form , intensify , propagate , and dissipate , the speed of solution variations ( and hence also of the mesh points ) in a given computation may vary a great deal . by no meansare we suggesting that a variable is necessary in all moving mesh calculations .nonetheless , there is some advantage to be gained by having an algorithm that is capable of determining the value of automatically as part of the solution process without requiring the user to determine its value through trial and error ( since the complicated nonlinear coupling between solution and mesh in the mmpde approach means there is no way to know the value of _ a priori _ ) .the main purpose of this paper is to demonstrate , by means of specific examples , that varying the mesh relaxation parameter throughout a computation can be of significant advantage in terms of both accuracy and efficiency .we will present an approach for adaptively selecting in such a way that the temporal evolution of the mesh is optimal in an appropriate sense .this paper is organized as follows . in section[ sec : mmpde ] , we briefly review moving mesh methods in which the mesh equation incorporates a relaxation time .the main motivating example for introducing an adaptive strategy for choosing a time - dependent mesh smoothing parameter comes from a class of nonlinear parabolic equations exhibiting self - similar blow - up behaviour ; we therefore introduce in section [ sec : blowup ] the blow - up model equation , and motivate a particular choice of which is suggested by the analysis of blow - up problems .numerical experiments are then presented in section [ sec : results ] to illustrate the advantages of this modified moving mesh approach in terms of both accuracy and efficiency .the evolution of a moving computational grid can be viewed as a discretization of a one - to - one , time - dependent coordinate mapping .let and denote the physical and computational coordinates respectively , and define a coordinate transformation by where both and are assumed to lie in interval  ] and  ] , which forces to lie in the interval  ] .one aim of these computations is to compute as far into blow - up as possible and to obtain the best possible estimate of the blow - up time . in all our simulations, we compute as far as ddassl will allow , up until such time as the solver fails ( which in practice manifests itself as a time step selection failure ) .since no exact analytical solution is available for this problem , it is difficult to assess the accuracy of a given computed solution . in this paper , we employ a number of qualitative and quantitative measures to compare the accuracy of the computed solutions :  * the termination time ( as an estimate of ) is compared to the blow - up time determined from a highly - resolved calculation , which gives a combined measure of the accuracy of the solution and the mesh . in particular , our best estimates of the blow - up times are  for and  for , both of which are calculated with points , variable with , and ddassl error tolerances of . * the value of , which is an indirect measure of solution accuracy , that represents how far the code is capable of computing into the singularity .ideally , we aim for to be as large as possible . *the self - similarity of the various solution profiles computed over time is most easily determined by comparing directly to the following asymptotic formula derived in :   the computational cost of all subsequent simulations is compared by measuring the elapsed cpu time on a 3 ghz intel xeon machine .we begin first by simulating the blow - up problem ( [ eq : blowup ] ) with .for all computations in this section , we have not performed any mesh smoothing ( i.e. , ) in order to ensure that the computed solution and mesh are as close to the similarity solution as possible . the solution andmesh contour plots for points are displayed for a constant value of in figure [ fig : p2const ] .the various solution profiles correspond to a sequence of snapshots at times where , .the plot of demonstrates how mmpde6 with the monitor is capable of capturing the self - similar nature of the solution .[ c][b] [ bc][c] [ c][b] [ bc][c]  and in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  and in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  to illustrate the effect of the choice of mmpde on the solution , we have also displayed the results for the same input data using mmpde4 in figure [ fig : p2const - mmpde4 ] .this computation is clearly incapable of maintaining grid resolution within the blow - up peak ; in fact , by the end of the calculation , the mesh degenerates to the extent that there remains only a single grid point left to resolve the peak .furthermore , this simulation fails at time , which is a much less accurate estimate of the blow - up time than in the mmpde6 calculations , as we will see shortly .consequently , mmpde6 is employed in the remainder of the simulations in this paper .[ c][b] [ bc][c] [ c][b] [ bc][c] .,title=""fig:"",scaledwidth=40.0% ] .,title=""fig:"",scaledwidth=40.0% ]  we next consider the effect of varying the mesh relaxation parameter by selecting two constant values ( and ) as well as varying according to our strategy outlined in section [ sec : varytau ] . the variable results are presented for comparison purposes in figure [ fig : p2var ] .there is clearly some loss of self - similarity in the solution profile relative to the constant simulations , which leads to considerably more mesh points being located outside the blow - up peak ; however , the peak is still reasonably well - resolved , and there are indeed a number of other reasons that the variable- results are superior , which we discuss next .[ c][b] [ bc][c] [ c][b] [ bc][c]  and variable in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  and variable in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  first , we performed a grid refinement study by varying between 40 and 600 , and compared the estimated blow - up times for all choices of in figure [ fig : p2tstar ] .first of all , the constant result with points is consistent with the value of reported in .the results require the least cpu time because such excessive temporal smoothing acts to reduce the stiffness in the mesh equation ; however , the estimate of is much less accurate and does not converge to the correct blow - up time as the other simulations do . among the remaining results ( and variable ) , there is no visible difference in the blow - up time seems to suggest no advantage in terms of accuracy .[ bc][c] [ c][b] [ bc][c]_cpu secs . _  in the case .the best estimate of the exact value of is shown as a dashed line.,title=""fig:"",scaledwidth=40.0% ]  in the case .the best estimate of the exact value of is shown as a dashed line.,title=""fig:"",scaledwidth=40.0% ]  nonetheless , the variable approach is still capable of computing further into the blow - up peak as evidenced by the maximum solution value : for , all values of lie between and while for the variable computations is always above at the end time .there is only a slight loss of self - similarity in the variable calculation , which can be seen by comparing the solution with the asymptotic profile ( [ eq : ucos ] ) , displayed as a dashed line in figures [ fig : p2const][fig : p2var ] .the primary advantage of the variable approach is in terms of efficiency , owing to the enhancement in temporal smoothing that occurs close to the blow - up time .the cpu time required in the variable case is consistently smaller by at least a factor of three relative to the constant computations , as depicted in figure [ fig : p2tstar ] .the variation of with time is shown in figure [ fig : p2var - tau ] , which demonstrates a nearly linear dependence as the blow - up point is approached . as a result, we can think of the variable algorithm as keeping the mesh relaxation time small when it is most needed ( at the time when the blow - up peak is first forming ) , but then introducing significant temporal smooting closer to the blow - up time when the mesh equation is most stiff , even though the mesh points themselves are not moving appreciably .[ bc][c] [ c][b] , for the same parameters as for the problem with variable .,title=""fig:"",scaledwidth=40.0% ]  in summary , the use of a variable permits a more accurate computation of both the blow - up time and the solution evolution , while still maintaining a reasonable degree of self - similarity in the solution , and all this at a significant savings in computational cost .the primary reason for the improvement in performance is the reduction in stiffness of the moving mesh pde which results from allowing the mesh relaxation time to vary .the blow - up problem constitutes a more difficult computational problem , and so we consider it a more stringent test of our moving mesh approach .in this case , as in  , we had to introduce mesh smoothing ( , ) in order to ensure stability of the mesh equation .proceeding as we did in the previous section , we compare the results to those for variable , and the results are depicted in figures [ fig : p5const ] and [ fig : p5var ] .  [ c][b] [ bc][c] [ c][b] [ bc][c]  blow - up solution profiles ( left ) and mesh contours ( right ) for fixed with .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  blow - up solution profiles ( left ) and mesh contours ( right ) for fixed with .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  [ c][b] [ bc][c] [ c][b] [ bc][c]  blow - up solution profiles ( left ) and mesh contours ( right ) for variable in the case . the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  blow - up solution profiles ( left ) and mesh contours ( right ) for variable in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  the constant computation exhibits oscillations in the solution which cause the integration to fail due to numerical instability .the variable- results , on the other hand , show no such instability , although the deviation from self - similarity is more significant than in the case .nonetheless , the mesh points are still reasonably well - clustered within the blow - up peak .there is a similar three - fold improvement in efficiency with the variable approach ( see figure [ fig : p5tstar ] ) although in this case the cpu times are nt as meaningful because the constant computations fail owing to mesh instability .again , we see the superiority of our adaptive approach for solving blow - up problems .  the estimated blow - up times are plotted in figure [ fig : p5tstar ] , which demonstrate further the instability experienced with the constant computations .the blow - up time for the variable result converges nearly monotonically and we claim it is a much more accurate estimate of the actual blow - up time for the calculation .[ bc][c] [ c][b] [ bc][c]_cpu secs . _ .the best estimate of ( computed with points and increased ddassl tolerances ) is shown as a dashed line.,title=""fig:"",scaledwidth=40.0% ] .the best estimate of ( computed with points and increased ddassl tolerances ) is shown as a dashed line.,title=""fig:"",scaledwidth=40.0% ]   the following blow - up problem with an exponential nonlinearity was also considered in and is an even more difficult test of the moving mesh method .the appropriate monitor function to use in this case is , and in analogy with the derivation of ( [ eq : ucos ] ) , there exists an asymptotically self - similar profile we start with initial data , and use mmpde6 with spatial smoothing parameter .  the results for mesh points are displayed in figures [ fig : euconst ] and [ fig : euvar ] .there is a slight loss of self - similarity in both cases owing to the introduction of spatial smoothing , but the difference between the two solutions is minimal .the primary difference is in terms of efficiency , where the variable- simulation requires consistently 30% less cpu time than for fixed .this is not as dramatic an improvement as for the polynomial blow - up examples considered in the previous two sections , as can be seen in figure [ fig : eucpu ] , but it is still a significant improvement .[ c][b] [ bc][c] [ c][b] [ bc][c]  with .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  with .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  [ c][b] [ bc][c] [ c][b] [ bc][c]  in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  in the case .the self - similar profile is displayed as a dashed line for comparison.,title=""fig:"",scaledwidth=40.0% ]  [ c][b] [ bc][c]_cpu secs . _in this paper , we have considered a moving mesh approach for solving self - similar blow - up problems .the novelty of this method stems from its use of a solution - dependent mesh relaxation time , .we have proposed a strategy for selecting in the context of self - similar blow - up problems .numerical simulations demonstrate that by varying the relaxation time in an appropriate way , the solution can be computed more accurately , further into the blow - up , and more efficiently than would otherwise be possible with a constant value of .because our strategy for adapting is specific to problems of blow - up type , we plan in the future to extend these results by generalizing them to a more generic class of problems .we intend to investigate other nonlinear parabolic problems that exhibit more general blow - up behaviour ( such as the generalized korteweg - de vries or gierer - meinhardt equations ) as well as problems with moving fronts .we briefly redo the analysis from for time - dependent .we begin with ( [ eq : mmesh - tau ] ) and perform a taylor series expansion for small to obtain : substituting these expressions into ( [ eq : mmesh - tau ] ) we obtain the corresponding equations for mmpde4 and mmpde6 respectively : notice that relative to ( [ eq : mmpde4 ] ) and ( [ eq : mmpde6 ] ) , the only change here is an extra factor of which simply scales . therefore a time - dependent has a minimal impact on the moving mesh equation .        c. j. budd ,j. chen , w. huang , and r. d. russell , moving mesh methods with applications to blow - up problems for pdes , in : d. f. griffiths and g. a. watson ( eds . ) , _ proceedings of 1995 biennial conference on numerical analysis , _ pitman research notes in mathematics vol .344 , addison wesley , 1996 , pp ."," we propose a moving mesh adaptive approach for solving time - dependent partial differential equations . the motion of spatial grid points is governed by a moving mesh pde ( mmpde ) in which a _ mesh relaxation time _ is employed as a regularization parameter . previously reported results on mmpdes have invariably employed a constant value of the parameter . we extend this standard approach by incorporating a variable relaxation time that is calculated adaptively alongside the solution in order to regularize the mesh appropriately throughout a computation . we focus on singular problems involving self - similar blow - up to demonstrate the advantages of using a variable relaxation time over a fixed one in terms of accuracy , stability and efficiency .   _ ams classification : _ 65m50 , 65m06 , 35k57  and  moving mesh method ; self - similar blow - up ; relaxation time "
"feature selection is an important aspect in the implementation of machine learning methods .the appropriate selection of informative features can reduce generalisation error as well as the storage and processing requirements for large datasets .in addition , parsimonious models can provide valuable insight into the relations underlying elements of the process under examination .there is a wealth of literature on the subject of feature selection when the relationship between variables is linear .unfortunately when the relation is non - linear feature selection becomes substantially more nuanced .kernel methods excel in modelling non - linear relations .unsurprisingly , a number of kernel - based feature selection algorithms have been proposed .early propositions , such as recursive feature elimination(rfe ) [ 1 ] can be computationally prohibitive , while attempts to learn a convex combination of low - rank kernels may fail to encapsulate nonlinearities in the underlying relation .recent approaches using explicit kernel approximations can capture non - linear relations , but increase the storage and computational requirements .the successful use of a kernel - based feature selection methods is a matter of balance .our approach makes extensive use of _ kernel target alignment _ ( kta ) [ 2,3 ] .work in [ 4 ] provides the foundation of using the alignment of centred kernel matrices as the basis for measuring statistical dependence .the hilbert - schmidt independence criterion is the basis for further work in [ 5 ] , where greedy optimisation of centred alignment is employed for feature selection .additionally , [ 5 ] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework .stability selection [ 6 ] is a general framework for variable selection and structure estimation of high dimensional data .the core principle of stability selection is to combine subsampling with a sparse variable selection algorithm . by repeated estimation over a number of different subsamples, the framework keeps track of the number of times each variable was used , thus maintaining an estimate for the importance of each feature .more importantly , stability selection provides finite sample control for some error rates of false discoveries and hence a principled approach for parameter selection . in this work, we propose a synthesis of the two aforementioned approaches through a randomised feature selection algorithm based on estimating the statistical dependence between bootstrapped random subspaces of the dataset in rkhs .the dependence estimation of random subsets of variables is similar to the approach of [ 13 ] , which is extended through bootstrapping and carefully controlled feature set sizes .this approach is simple to implement and compares favourably with other methods in terms of scalability .the rest of the paper is structured as follows : _ section 2 _ presents the necessary background on feature selection for kernel - based learning ._ section 3 _ introduces a basic randomised algorithm for nonlinear feature selection , along with some simple examples , while _ section 4 _ provides some analysis .extensive experimentation on real and artificial data in _ section 5 _ concludes this paper .we consider the supervised learning problem of modelling the relationship between a input matrix and a corresponding output matrix .the simplest instance of such a problem is binary classification where the objective is the learning problem is to learn a function mapping input vectors to the desired outputs . in the binary casewe are presented with a matrix and a vector of outputs , limiting the class of discrimination functions to linear classifiers we wish to find a classifier   the linear learning formulation can be generalised to the nonlinear setting through the use of a nonlinear feature map , leading to the kernelized formulation :   the key quantities of interest in our approach is the centred kernel target alignment which is defined as :   the matrices and correspond to centred kernels on the features and outputs and are computed as :   k \left [ i-{{11^t}\over m } \right]\ ] ]  where , in the above equation denotes the m - dimensional vector with all entries set equal to one .the approach we will take will be based on the following well - known observation that links kernel target alignment with the degree to which an input space contains a linear projection that correlates with the target .let be a probability distribution on the product space , where has a projection into a hilbert space defined by a kernel .we have that } = & & \\ & & \hspace{-5cm}= \sup_{{\mathbf{w } } : \|{\mathbf{w}}\| \leq 1 } { \mathbb{e}}_{({\mathbf{x}},y)\sim p}[y\langle{\mathbf{w}},\phi({\mathbf{x}})\rangle]\end{aligned}\ ] ]  * proof : * = & & \\ & & \hspace*{-5cm}= \sup_{{\mathbf{w } } : \|{\mathbf{w}}\| \leq 1 }\left\langle{\mathbf{w}},{\mathbb{e}}_{({\mathbf{x}},y)\sim p}[\phi({\mathbf{x } } ) y]\right\rangle \\ & & \hspace*{-5cm}= \left\| { \mathbb{e}}_{({\mathbf{x}},y)\sim p}[\phi({\mathbf{x } } ) y]\right\|\\ & & \hspace*{-5cm}=\sqrt { \int \int dp({\mathbf{x}},y)dp({\mathbf{x}}',y ' ) \langle \phi({\mathbf{x}}),\phi({\mathbf{x}}')\rangle  yy'}\\ & & \hspace*{-5cm}= \sqrt{{\mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[y y ' \kappa({\mathbf{x}},{\mathbf{x } } ' ) ] } \end{aligned}\ ] ]  the proposition suggests that we can detect useful representations by measuring kernel target alignment . for non - linear functions the difficulty is to identify which combination of features creates a useful representation .we tackle this problem by sampling subsets of features and assessing whether on average the presence of a particular feature contributes to an increase in the average kernel target alignment . in this way we derive an empirical estimate of a quantitywe will term the contribution .  the _ contribution _ of feature is defined as \right ] -  { \mathbb{e}}_{s ' \sim { \mathcal{s}}_{\setminus i}}\left[{\mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[y y ' \kappa_{s'}({\mathbf{x}},{\mathbf{x}}')]\right],\ ] ] where denotes the ( non - linear ) kernel using features in the set ( in our case this will be a gaussian kernel with equal width ) , the uniform distribution over sets of features of size that include the feature , the uniform distribution over sets of features of size that do not contain the feature , and is the number of features .  note that the two distributions over features and are matched in the sense that for each with non - zero probability in , has equal probability in .this approach is a straightforward extension of the idea of bahsic [ 5 ] .we will show that for variables that are independent of the target this contribution will be negative . on the other hand ,provided there are combinations of variables including the given variable that can generate significant correlations then the contribution of the variable will be positive .we will define an _ irrelevant _ feature to be one whose value is statistically independent of the label and of the other features .we would like an assurance that irrelevant features do not increase alignment .this is guaranteed for the gaussian kernel by the following result .let be a probability distribution on the product space , where has a projection into a hilbert space defined by the gaussian kernel on a set of features .suppose a feature is irrelevant .we have that \leq { \mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[y y ' \kappa_{s}({\mathbf{x}},{\mathbf{x}}')]\ ] ]  * proof ( sketch ) : * since the feature is independent of the target and the other features , functions of these features are also independent .hence , & & = { \mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[y y ' \kappa_{s}({\mathbf{x}},{\mathbf{x}}')\exp(- \gamma ( x_i - x'_i)^2)]\\ & & \hspace*{-2.5cm}= { \mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[y y ' \kappa_{s}({\mathbf{x}},{\mathbf{x}}')]{\mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[\exp(- \gamma ( x_i - x'_i)^2)]\\ & & \hspace*{-2.5cm}= { \mathbb{e}}_{({\mathbf{x}},y ) \sim p , ( { \mathbf{x } } ' , y')\sim p}[y y ' \kappa_{s}({\mathbf{x}},{\mathbf{x}}')]\alpha\end{aligned}\ ] ] for \leq 1 $ ] .  in fact the quantity is typically less than 1 so that adding irrelevant features decreases the alignment . our approach will be to progressively remove sets of features that are deemed to be irrelevant , hence increasing the alignment together with the signal to noise ratio for the relevant features .figure [ xor ] shows how progressively removing features from a learning problem whose output is the xor function of the first two features both increases the alignment contributions and helps to highlight the two relevant features .t    we now introduce our definition of a relevant feature .a feature will be termed -_influential _ when its contribution .so far have only considered expected alignment . in practice we must estimate this expectation from a finite sample .we will omit this part of the analysis as it is a straigthforward application of u - statistics that ensures that with high probability for a sufficiently large sample from and and of samples from ( whose sizes depend on , , the number of -influential variables and the number of iterations ) an empirical estimate of the contribution of an -influential variable will with probability at least be greater than 0 for all of the fixed number of iterations of the algorithm .our final ingredient is a method of removing irrelevant features that we will term culling . at each iteration of the algorithmthe contributions of all of the features are estimated using the required sample size and the contributions are sorted .we then remove the bottom 25% of the features in this ordering .our main result assures us that culling will work under the assumption that the irrelevant variables are independent .fix .suppose that there are -influential variables and all other variables are irrelevant .fix and number of iterations . given sufficiently many samplesas described above the culling algorithm will with probability at least remove only irrelevant variables will be removed .* proof ( sketch ) : * each irrelevant variable has expected contribution less than the contributions of the all the influential features .hence , with high probability at least 30% of these features will have lower contributions than all the influential features .hence , the bottom 25% will all be irrelevant .we now define our algorithm for randomised selection ( randsel ) . given a input matrix and corresponding output matrix , randsel proceeds by estimating the individual contribution of features by estimating the alignment of a number of random subsamples that include and randomly selected features .this leads to an estimate for the expected alignment contribution of including a feature .the algorithm is parametrized by the number of bootstraps , a bootstrap size and a percentage of features that are dropped after bootstraps .the algorithm proceeds iteratively until only two features remain .optionally the algorithm can be further parametrized by permanently including features which were ranked in the top percentile on at least a number occasions .this option enhances the probability of detecting non - linear dependencies between variables , should they be present .there are a number of benefits to this approach , aside from the tangible probabilistic guarantees .randsel scales gracefully . considering the computation of a kernel for samples atomic ,the number of kernel computations for a single iteration are , which for a sensible choice of can be substantially smaller than the complexity of hsic variants .for example setting and an iteration would require kernel element computations , and in addition this process is trivial to parallelize .input data , labels , number of iterations subsample size , number of features , drop percentile proportion , top percentile proportion , number of occasions = random subsample of size over randomly selected variables = alignment( ) = random subsample of size over randomly selected variables = alignment( ) mean contribution = mean ( ) - mean( ) , where and drop the % bottom - contributing features save the % top - contributing features fix feature return sequence of estimated contributions and fixed variablesin this section , we present several experiments comparing our feature selection approach to a number of competing methodologies .we have used three synthetic datasets in order to better illustrate the performance characteristics of these algorithms before proceeding to experiments on real data arising in infectious disease profiling .   in both synthetic and real datasetswe used nested 10-fold cross validation to perform feature selection , and repeated the simulations on three different reshuffles of the dataset to account for variance . for every iteration we estimate the validation error after feature selection before proceeding to test on the held out test - setthe inner cross - validation loop determines the number of features to use in classifying the test - set for optimal accuracy . if two or more models are tied in terms of performance , the more parsimonious model is preferred .we compare our proposed approach to kernel based algorithms like * rfe * , * fohsic * and * bahsic * , as well as a filtering approach relying on * correlation coefficients * and * stability selection * using the lasso as the underlying sparse selection algorithm . the same range of gaussian kernel bandwidths was explored in all algorithms and the resulting final classifiers employed a regularisation parameter of .we generated three synthetic datasets in order to carefully illustrate the properties of the different feature selection algorithms .all three synthetic datasets contain 300 samples with a dimensionality of 100 features .the linear and non - linear weston datasets were generated according to [ 7 ] , and consist of 5 relevant and 95 noise features .neither the linear or non - linear weston datasets exhibit a nonlinear interdependence between features .we produced a simple xor pattern dataset in order to simulate this scenario .along with the accuracy on the test set and the sparsity we also record the precision and recall of the selection algorithms .analogously to information retrieval , we define the precision as the number of the relevant features that were selected from the feature selection procedure over the total number of features selected and recall as the number of relevant features selected over the total number of relevant features ..results on synthetic data .[ cols= "" < , < , < , < , < , < , < , < , < "" , ]   for the mass spectrometry tasks we used randsel using 5000 bootstraps of size of the dataset , culling the bottom of variables in terms of expected contribution after the end of each iteration .we used similar parameters for the micro - array dataset but with an increased number of bootstraps of 10000 in order to account for the substantially higher dimensionality of the data .again , no variables were fixed and the algorithm iterated until only two variables remained . in _ task 1 _ , randomized selection is tied with stability selection in terms of accuracy , however on average the randomised recovered feature set is significantly sparser .interestingly , simple filtering based on correlation coefficients performs strongly in the mass spectrometry tasks , often beating the hsic variants and in fact gives the highest accuracy for _ task 2_. the only test where all the hsic variants outperformed correlation filtering is the microarray task , which has a substantially increased dimensionality in comparison to the mass spectrometry datasets .the results indicate that the hsic - based variants ( randsel , fohsic & bahsic ) often recover sparser solutions compared to competing algorithms .given their mutual reliance on hsic optimisation , the fact that randsel outperforms the other methods in terms of accuracy can be surprising at first glance .it is instructive at this point to acknowledge that these methods rely on heuristics to solve an np - hard problem .the synthetic xor dataset already underlines one scenario where randsel outperforms forward greedy selection .the results on real data , combined with our theoretical guarantees , suggest the possibility of arriving at an improved global solution through randsel s incorporation of stochastic information , in opposition to the strategy of obliviously eliminating the locally optimal variable employed in bahsic .   a final experimental application where we employed randomised selection was in the recent black box learning challenge [ 10][13 ] .after performing an initial unsupervised feature learning step on the original dataset using sparse filtering [ 11 ] , we performed randomised selection in the resulting representation , creating kernels corresponding to the remaining features after each iteration of the feature selection algorithm . treating each kernel as defining a class of weak learners , we used lpboost to perform multiple kernel learning ( lpboostmkl [ 12 ] ) .the resulting classifier beat many of our other approaches and was one of the strongest performers in the challenge .in this paper we propose randsel , a new algorithm for non - linear feature selection based on randomised estimates of hsic .randsel , stochastically estimates the expected importance of features at each iteration , proceeding to cull features uninformative features at the end of each iteration .our theoretical analysis gives strong guarantees for the expected performance of this procedure which is further demonstrated by testing on a number of real and artificial datasets .this , combined with the algorithm s attractive scaling properties make randsel a strong proposition for use in application areas such as quantitative biology , where the volume of data increases at a frantic pace .goodfellow , i. j. , erhan , d. , carrier , p. l. , courville , a. , mirza , m. , hamner , b. , ... & bengio , y. ( 2013 ) challenges in representation learning : a report on three machine learning contests .proceedings of the 20th international conference on neural information processing"," feature selection plays a pivotal role in learning , particularly in areas were parsimonious features can provide insight into the underlying process , such as biology . recent approaches for non - linear feature selection employing greedy optimisation of centred kernel target alignment(kta ) , while exhibiting strong results in terms of generalisation accuracy and sparsity , can become computationally prohibitive for high - dimensional datasets . we propose randsel , a randomised feature selection algorithm , with attractive scaling properties . our theoretical analysis of randsel provides strong probabilistic guarantees for the correct identification of relevant features . experimental results on real and artificial data , show that the method successfully identifies effective features , performing better than a number of competitive approaches .  ....   feature selection , kernels .... "
"modified - electroconvulsive therapy ( m - ect ) is administered for the treatment of various psychiatric disorders .the seizure generalization hypothesis , which underlies the mechanism of ect , holds that propagation of induced seizure throughout the entire brain is essential for effective ect intervention .however , there are many clinical cases where , due to high thresholds , seizure is not induced by the maximum dose of electrical charge . in these cases ,the following procedures are considered options for inducing seizure ; ( i ) using the older method of sine - wave ect ( ii ) promoting hyperventilation in patients ( iii ) using anesthetic agents such as ketamine with ect .however , these are not standard methods as sine - wave ect induces more severe side effects than pulse - wave ect , and not all anesthesiologists are fully trained in the latter two procedures .recently randomized control trials focusing on pulse width have been conducted .sackeim et al reported that the ultrabrief pulse method , in which pulse width is less than 0.3millisecond ( ms ) , induces more therapeutic effects and fewer side effects and requires less electrical charge to induce seizure compared to conventional brief pulse ( 1.5ms ) .it could be predicted then that the ultrabrief pulse would be more effective in inducing seizure in patients with high thresholds .contrary to this , we experienced a case of schizophrenia in which m - ect with 1.0 and 1.5ms width pulse ( referred to as ` long ' brief pulses as 0.5ms width pulse is the default in japan ) succeeded in inducing seizure , whereas ultrabrief pulse failed .we present this case in detail and discuss the possible underlying mechanisms .written informed consent was obtained from the patient and his wife ( legal guardian ) .all personal information has been anonymized .the patient is a 35-year - old schizophrenic japanese male .his history of illness started at age 23 with symptoms of auditory hallucinations , persecutory delusions and psychomotor excitement .subsequently these symptoms relapsed every one to three years .he was discharged from hospital three years ago and was being followed as an outpatient .one month ago , he complained to his doctor that he was ` being watched by strangers ' .the atypical antipsychotic blonanserine and the mood stabilizer valproate were added to his medication but these produced the persecutory delusion that the new therapy was part of an experiment instigated by his doctor .he became aggressive with verbal threats and was admitted to our hospital due to a lack of available beds in his regular treating hospital .  on admission, he was psychotic and extremely agitated , and needed to be restrained .he initially refused treatment but eventually agreed to his old treatment regime excluding blonanserine and valproate .his medical history revealed that medication was of little effect for his relapsed symptoms while m - ect was effective , although sine - wave ect was necessary due to his high threshold .the presence of brain disease such as tumours was excluded after review of computed - tomography images .ect treatment was instigated three days post admission .for the ect interventions , we used a somatic thymatron ect device , placing electrodes in a bitemporal configuration and administering propofol at 1.0mg / kg as an anesthetic induction and succinylcholin at 1.0 mg / kg as a muscle relaxant .figure 1 shows the clinical course of this patient . in trials 1 to 3, we used a ` low 0.5 ' setting , in which the pulse width is fixed at 0.5ms , but this failed to induce seizure at the maximum dose of electrical charge ( 504 milicoulomb ) .we then changed the thymatron setting from ` low 0.5 ' to ` low 0.25 ' , with pulse width fixed at 0.25 ms , for trial 4 , performed immediately after trial 3 .contrary to expectations , this did not induce seizure . in trial 5 ,a setting of ` low 0.5 ' succeeded in inducing 9-second seizure , recognized by eeg and emg charts .this seizure , however , was deemed insufficient due to its short duration time .we then changed pulse width settings to 1.0ms and administered trial 6 .this setting successfully induced seizure with the desirable waveform and of sufficient duration ( fig 2 ) .  for the remainder of the treatment we administered ect , decreasing the electrical charge to avoid side effects .at the 1.0ms pulse width , we achieved desirable seizures with 60 percent of the maximal charge . at the 1.5ms pulse width , we succeeded in inducing seizure at 40 percent of the maximal charge .we stopped ect treatment after 11 trials as his psychiatric condition had improved considerably from the day of admission .he was then transferred to a chronic ward and prepared for discharge .for this patient , all trials at 0.25ms width pulse failed to induce seizures while 0.5ms width pulse was successful only once ( in trial 5 ) .it is likely that falling serum valproate concentrations enabled seizure induction in trial 5 whereas the same pulse width failed to induce convulsions in the earlier trials ( 1 through 3 ) .the 1.0ms width pulse succeeded in inducing therapeutic seizure with desirable waveform , while the 1.5ms width pulse trials induced seizure of unknown clinical effect .  in summary ,long brief pulse was more effective for inducing seizure than ultrabrief pulse for this patient . taken together with recent rct studies , this case suggests that seizure threshold depends on pulse width .however , results are contradictory . in the rct studies ,ultrabrief pulse was more effective than long brief pulse for inducing seizures but , in this case , the reverse was true .recent rct studies of ultrabrief pulses are based on the electrophysiological fact that the chronaxie ( the most effective pulse width in firing neuron ) of neurons in the mammalian central nervous system lies within 0.1 - 0.3ms .however , west et al reported that the strength - duration curve of one - thirds of neurons is right - shifted , even in normal subjects , so that their chronaxie is prolonged .we speculate that in our patient the strength - duration curve involved in ect - induced seizures might be right - shifted resulting in prolonged chronaxie through to about 1.0ms .figure 3 shows a possible mechanistic explanation for this apparent contradiction .moreover , this view accords well with the fact that there is little clinical difference between a 0.25ms and 0.5ms width pulse . if the strength - duration curve is not right - shifted ( in an individual , or where sample size is such that this can be assumed ) , then the difference in electrophysiological response produced by a pulse width of a 0.25ms compared to 0.50ms is negligible relative to the difference in response between 0.30ms compared to 1.5ms .although our hypothesis is likely to bridge clinical usage of ect and relevant fundamental research , it is first necessary to confirm whether our observations are limited to this case or are applicable to a wider group of patients with careful ect treatment .we thank the medical staff at matsuzawa hospital especially dr okazaki .9  abrams r. electroconvulsive therapy .oxford  university press , ny , 2002 loo c et al .`` augmentation strategies in electroconvulsive therapy . ''j ect , 26 ; 202 - 207 , 2010 sackeim ha et al .`` effects of pulse width and electrode placement on the efficacy and cognitive effects of electroconvulsive therapy . ''brain stimulation , 1 ; 71 - 83 , 2008 loo c et al .`` a comparision of rul ultrabrief pulse ( 0.3ms ) ect and standard rul ect . ''int j neuropsychopharmacol , 11 ; 883 - 890 , 2008 peterchev av et al .`` ect stimulus parameters : rethinking dosage . ''j ect , 26 ; 159 - 174 , 2010 west dc et al. `` strength - duration characteristics of myelinated and non - myelinated bulbospinal axon in the cat spinal cord '' .j physiol , 337 ; 37 - 50 , 1983 niemantsverdriet l et al .`` the efficacy of ultrabrief - pulse ( 0.25ms ) versus brief - pulse ( 0.50ms ) bilateral electroconvulsive therapy in major depression . ''j ect , 27 ; 55 - 58 , 2011"," modified - electroconvulsive therapy ( m - ect ) is administered for the treatment of various psychiatric disorders . the seizure generalization hypothesis holds that propagation of the induced seizure throughout the whole brain is essential for the effective ect intervention . however , we encounter many clinical cases where , due to high thresholds , seizure is not induced by the maximum dose of electrical charge . some studies have indicated that the ultrabrief pulse method , in which pulse width is less than 0.5millisecond ( ms ) , is more effective at inducing seizure than conventional brief pulse ( 0.5ms-2.0ms ) . contrary to the studies , we experienced a case of schizophrenia in which m - ect with 1.0 and 1.5 ms width pulse ( referred to as ` long ' brief pulse as 0.5ms width pulse is the default in japan ) succeeded in inducing seizure , whereas ultrabrief pulse failed to induce seizure . this case is described in detail . moreover , we discuss the underlying mechanism of this phenomenon . "
"ontologies are witnessing an increasing popularity outside specialized ai communities .while this is mostly due to semantic web applications  , we must also credit their ability to cope with taxonomies and part - whole relationships , to handle heterogeneous attributes , and their provision for various automated reasoning services see , e.g. ,  .these features have been recognized since long time in system engineering , the community encompassing all areas of research devoted to design , implementation , monitoring and diagnosis of technical processes .for instance , in the operations and maintenance sub - community , the use of ontologies is explicitly advocated . also standards like iso 13374 (_ condition monitoring and diagnostics of machines data processing , communication and presentation _ ) suggest the use of ontologies for several tasks , mostly related to data conceptualization .however , the adoption of ontologies faces some challenges , mostly due to speed and reliability constraints imposed by industrial settings .  herewe investigate this issue by considering four contributions of ours to application domains wherein ontologies provide key capabilities in system engineering .the first case study is about an on - board rolling - stock condition analyzer , i.e. , a system to perform fault detection and classification  .the second one is about monitoring an intermodal logistic system  .the third one is about an ontology - based framework to generate diagnostic - decision support systems  .finally , a fourth case study is an application to computer - automated design of elevator systems . in the following, we briefly introduce each case study , giving details about its context , underlying motivation and intended objectives .the ultimate goal of the paper is to discuss and compare the results obtained to assess the effectiveness of ontologies in such application domains .  _ontologies for condition analysis ._ we introduced an ontology - based condition analyzer ( ca )  in the context of the eu project integrail .our ca collects signals from control logic installed on locomotives , and it leverages an ontology to correlate observed data , symptoms and faults . the ca must mate two competing needs : railway regulations require hardware which is highly reliable , and whose performances are thus far even from desktop workstations ; ontology - related tools , e.g. , description logic reasoners , have relatively large memory , processor and storage footprints . in this experience , the main goal was thus to check whether reasoning with ontologies can provide useful diagnostic feedback in a resource - restricted scenario .  _ ontologies for system monitoring . _ in  we provided strong evidence of practical uses for ontologies in complex systems engineering by implementing a monitor for _ intermodal logistics systems _ ( ilss ) , i.e. , systems supporting the movement of containerized goods .in particular , we considered combination of rail and road transport , where rail transport is provided by short - distance shuttle trains , and network coverage is achieved through connections at specialized terminals . in this experience ,the main goal was to gather data about terminal operations and compute global performances indicators , where access to data is mediated by an ontology ontology - based data access ( obda )  . here , unlike the ca case study , the ability to handle large amount of data is crucial , but reasoning is limited to sparql query answering .  _ontologies for diagnostic support system generation ._ diagnostic decision support systems ( ddsss ) help humans to infer the health status of physical systems . in  we introduced disegno for `` diagnostic server generation through ontology '' to generate customized ddsss .as in the ils monitoring case study , since it is expected that large quantities of data should be handled , the ontology language is restricted to those designed for tractable reasoning see , e.g. ,  . in this case ,ontology - based reasoning is not leveraged , as disegno generates relational databases from the domain ontology and then computes diagnostic rules with ptolemy ii  , an open - source software simulating actor - based models .  _ontologies for computer - automated design . _as mentioned in  , the first scientific report of intelligent computer - automated design ( cautod ) is the paper by kamentsky and liu  , who created a computer program for designing character - recognition logic circuits satisfying given hardware constraints . in mechanical design see , e.g. ,  the term usually refers to techniques that mitigate the effort in exploring alternative solutions for structural implements . in our liftcreate cautod program for elevator systems ,ontologies support intelligent design creation and optimization by managing detailed part - whole taxonomies , wherein different relations among components can be expressed .this case study provides thus yet another application of ontologies , mostly oriented to intelligent computation and data persistency .overall , the case studies considered witness the great flexibility that ontologies provide in handling diverse application scenarios , from condition analysis of locomotives , to automated design of elevators , considering both cases wherein they provide the basis for logic reasoning services , or just advanced data - modeling capabilities . the rest of the paper is structured as follows . in sections [ sec : condition ] , [ sec : ilog ] , [ sec : ondabrief ] and [ sec : elevator ] we sketch the design , the implementation and the results obtained in the case studies described above .section [ sec : concl ] concludes the paper by summarizing the results and providing some discussion thereof .a portion of the e414 ontology regarding traction faults .concepts are nodes and object properties are edges : white nodes are sp3a concepts , grey ones are e414-specific concepts . ]the ca prototype described in  focuses on fault detection on trenitalia e414 locomotive .the main task of the ca is to perform fault classification according to priority for maintenance , and impact on mission - related and safety - related aspects . here, we focus on traction groups as an example of subsystem that can generate a faulty condition . our ontology for the e414 locomotive is written in owl 2 language and it builds on the sp3a core ontology see  for details .in particular , the e414 ontology leverages the sp3a concepts of observationdata , i.e. , process variables , and observation , i.e. , sequences of observation data from which individuals of class symptom and fault arise .symptom individuals are related to observation individuals via the referstoobservation property and to fault individuals via the referstofault property .fault is a concept whose individuals are defined in terms of the necessary hassymptom relationship with symptom individuals .two subclasses of fault are defined : priorityfault and nonpriorityfault , with obvious meaning . in figure[ fig : e414ont ] we show a portion of the e414 ontology related to traction faults , where concepts have been specialized in subclasses whose individuals correspond to actual signals and subsystems . fault and symptom classification is obtained by a description logic ( dl ) reasoner considering the patterns observed .for instance , in the case of tractionhightemperatureobservation , three ranges of temperatures are defined that correspond to `` interesting '' patterns : from 70 to 80 degrees , from 80 to 130 degrees , and over 130 degrees .it is postulated that observations falling in the second and in the third ranges are to be considered mission critical , while the ones in the first category are only maintenance critical .a detailed description of the ca architecture can be found in  .here we provide some intuition on how the analyzer works considering high temperatures in the traction groups . when the temperature of a group is higher than 70 degrees for at least 3 consecutive samples read from the field bus , the ca starts tracking a potential anomalous pattern .once such a pattern is detected , the corresponding individuals in the classes tractionobservationdata and tractionhightemperatureobservation are recorded .symptom individuals are built along with all the properties required by the ontology specification .for example , if an observation of the class tractionhightemperatureobservation has been created , a specific individual tractionhightemperatureobservation is related to a new symptom individual by the referstoobservation property .fault individuals for each symptom individual are created together with the causedbysymptom property .fault as well as symptom individuals are built of generic type , leaving their classification to the dl reasoner .once the classification is done , the ca publishes the results , transmitting them to external agents .as an example , let us assume that is an individual of the class tractionhightemperatureobservation whose property isat is set to the constant _ 130degrees , is the symptom individual related to , and is the fault individual related to .the e414 ontology postulates that all symptoms such that the corresponding observation is an instance of tractionhightemperatureobservation related by isat to the constant _130degrees are also an instance of tractiontotalmissionimpactsymptom , which is a subclass of symptom .therefore , a reasoner can infer that belongs to missionrelatedsymptom ..[tab : results ] results with ( a ) lazy and ( b ) eager implementations of the ca . [ cols=""^,^,^,^"",options=""header "" , ]   out of the three sets of experiments performed in  , we report just those to ensure that the ca implementation fits the constraints . to this end , we ran several tests using different fault scenarios .table [ tab : results ] shows the results obtained by running the ca on four different scenarios the first includes no fault , the second includes only one fault , the third includes five contemporary faults , and the last 17 contemporary faults using two different configurations .configuration ( a ) is `` lazy '' , i.e. , it keeps all the individuals , while configuration ( b ) is `` eager '' , i.e. , it deletes individuals as soon as possible . as we can see in table[ tab : results ] , the eager version results in a great improvement over the lazy one , both in terms of memory consumption and in terms of computation time .in particular , in the second column of table [ tab : results ] we can notice that the eager version performs reasonably well , even in the fourth test case ( worst - case scenario ) .in the same scenario , the lazy version exceeds the amount of available memory . as we can see in the rightmost column of table [ tab : results ] , the amortized computation time over a single scenario decreases with the number of concurrent observations detected in the round .managing a round of samples without detected observations takes only 90 ms , which leaves enough time for other activities , and allows the ca to process all the incoming signals in due course .in  we provided evidence that ontology - based data access ( obda )  is of practical use in the context of _ intermodal logistics systems _ ( ilss ) .the investigation focuses on the opportunity to build a monitoring information system ( mis ) using obda instead of relational databases ( rdbs ) .the application scenario is an ils relying on a logic akin to computer networks , i.e. , frequent short - distance trains with a fixed composition and a predefined daily schedule to cover some geographical area. _ intermodal transport units _ ( itus ) enter the network at some terminal and travel to their destination according to a predefined route , usually boarding more than one train along the way .terminals collect itus from areas of approximately 150 km in radius in order to minimize road transport .the mis is a key enabler to minimize delivery time , maximize rolling - stock and network utilization and , ultimately , reduce the economic overhead of transportation for the final customer .the main goal of the mis is to compute _ key performance indicators _ ( kpis ) to perform tactical and strategical decision making about the network .  in figure[ fig : ilog_dot ] we present a graphical outline of the ontology at the heart of our obda solution to monitor the ils .the ontology ils ontology in the following is compliant with the owl 2 ql profile described in the official w3c s recommendation as _`` [ the sub - language of owl 2 ] aimed at applications that use very large volumes of instance data , and where query answering is the most important reasoning task.''_. given the ils application domain , owl 2 ql guarantees that conjunctive query answering and consistency checking can be implemented efficiently with respect to the size of data and ontology , respectively .the restrictions that owl 2 ql implies did not hamper the modeling accuracy of our ils ontology . in figure[ fig : ilog_dot ] we can pinpoint classes related to freight forwarding such as * customer * , i.e. , companies forwarding their goods through the network , * requestforwork * , i.e. , the main document witnessing that a given customer has issued a request for transporting a number of itus , * transportorder * , i.e. , the `` bill of transit '' associated to each itu , as well as entities related to physical elements such as * itu * , * terminal * and * train*. also `` logical '' entities are modeled such as * route * , i.e. , a sequence of terminals and railway connections serviced regularly by one or more scheduled trains and * scheduledstop * , i.e. , terminals associated to a given route with a given schedule . *event * is the main monitoring entity , as the calculation of most kpis relies on the exact recording of events at specific locations .  to assess obda performances , in  we obtained different artificial utilization scenarios by changing the number of itus shipped daily from each terminal .considering typical usage patterns , we postulated that a provision of 10 to 50 itus is to be shipped daily from each terminal , with 40 to 50 itus corresponding to a heavy utilization .scenarios are simulated for an increasing number of days to evaluate scalability , and all of them share common settings as far as number of train travels , number of cars per train , and timetabling are concerned .unexpected delays as well as the number of customers per terminal follow a probabilistic model see  for more details . in figure [ fig : results ] we display the results obtained in the case of an heavy utilization scenario to compute a specific kpi , namely the average number of itus unloaded per hour . the performance of four different query - answering systems are reported : a sql query on a native rdb implementation , and a sparql query on the ontology store .the sparql query can be answered by three different systems , namely arq ( the default query processor in the jena library ) , pellet ( the same dl reasoner that we consider in section [ sec : condition ] ) and quest  .the latter is the only reasoner exploiting the fact that sparql queries can be compiled on - the - fly into sql queries for an equivalent rdb representation of the ontology stored in the main memory . as we can observe in figure[ fig : results ] , obda - based solutions show higher overall computation times than the rdb - based solution from 1 to 2 orders of magnitude together with an apparently growing trend associated to the time span of the simulation .however , as we have shown in  , a trend test performed on the results obtained with the best obda solutions for various kpis , displays no statistically significant increase in the cpu time required to answer various queries with respect to the number of days . considering that for most kpis we can adopt an `` eager '' solution similar to that considered in section [ fig : results ], we can conclude that obda is practically feasible for monitoring medium - to - large scale systems .in  we introduced an approach to compile ontology - based descriptions of equipment into diagnostic decision support systems ( ddsss ) .the tool disegno , whose functional architecture and work - flow is sketched in figure [ fig : ondamodel ] , fulfills this task in three phases : in the phase , a domain ontology and diagnostic rules model are designed by the user ; in the phase , the system reads and analyzes the ontology and the rules to output the actual ddss ; in the phase , input web services receive data from the observed physical system and record them in the generated data store . according to the iso 13374 - 1 standarda ddss consists of six modules of which disegno implements three : _ data manipulation _ to perform signal analysis and compute meaningful descriptors , _ state detection _ to check conformity to reference patterns , and _ health assessment _ to diagnose faults and rate the current health of the equipment or process .as shown in figure [ fig : ondamodel ] , the ontology description is created by a system architect in the phase .the ontology must be written using owl 2 ql language as in the case study shown in section [ sec : ilog ] .the diagnostic computation model must be a sound actor diagram generated by ptolemy ii  which describes the processing to be applied to incoming data in order to generated diagnostic events here we focus on the ontology part , but more details on the rule handling part can be found in  .the phase contains the actual ddss generation system which consists of the , i.e. , a piece of software that creates a relational database by mapping the domain ontology to suitable tables , and the , i.e. , a module that creates interface services for incoming and outgoing events .finally , in the phase , data is acquired and stored in the internal database , the rules engine processes data and generates diagnostic events which are then served to some application .an example of a disegno - compliant equipment description is shown in figure [ fig : hvac_onto ] .the ontology is related to a heating ventilation and air conditioning ( hvac ) appliance and it is divided into a _static _ and a _ dynamic _ part . in the static part , which is not updated while monitoring, the ontology contains a description of the observed physical system . in the hvac ontologywe have * system * and * datasource * , related by the * isinsystem * property . * hassubsystem * relationship indicates that one * system * could be composed by one or more * systemcomponent * which are themselves subclasses of * system*. finally , * datasource * is the class of elements that can generate diagnostic - relevant information .the dynamic part describes _ events _ , including both the ones generated by the observed system and its components , and those output by the ddss . an event is always associated to a time - stamp and it can be either _ incoming _ to the ddss from the observed system , or _outgoing _ from the ddss .the main concepts in the dynamic part of the hvac ontology are * ddss * which * receives * instances of * incomingevent * and * sends * instances of * outgoingevent*. notice that * incomingevent * instances are connected to * datasource * instances by the role * generates * , denoting that all incoming events are generated by some data source .also every * outgoingevent * instance , i.e. , every diagnostic event , * relatesto * some instance of * datasource * , because the end user must be able to reconstruct which data source(s ) provided information that caused diagnostic rules to fire a given diagnostic event .* outgoingevent * specializes to * alarmevent * , * faultevent * and * descriptorevent*. every * outgoingevent * instance is connected to one of * diagnosticindicator * instances , i.e. * alarm * , * fault * and * descriptor * sub - concepts , by * reports * relation , in order to have a reference message about the diagnostic rules .+  our latest ontology - based application is in the field of computer - automated design ( cautod ) which differs from `` classical '' computer - aided design ( cad ) in that it is oriented to replace some of the designer s capabilities and not just to support a traditional work - flow with computer graphics and storage capabilities .nevertheless , cautod programs most often include cad facilities to visualize technical drawings related to the implements of interest .in particular , our liftcreate program is oriented to automating design of elevators , taking the designer from the very first measurements to a complete project which guarantees feasibility within a specific normative framework .liftcreate works in three steps . in the first step ,the user is asked to enter relevant parameters characterizing the project , and an overall `` design philosophy '' to be implemented .for instance , if the size of the elevator s shaft is known and fixed in advance , liftcreate can generate solutions which maximize payload , door size , or car size .a design philosophy is just a set of heuristics which , e.g. , prioritize door size over other elements , still keeping into account hard constraints , e.g. , payload and car size should not fall below some threshold . in the second phase, liftcreate retrieves components from a database of parts and explores the ( combinatorial ) space of potential solutions , either using heuristic search techniques , or resorting to optimizations techniques like those suggested , e.g. , in  . in the third phase , a set of feasible designsis proposed to the user , sorted according to decreasing relevance considering the initial design philosophy .for instance , if door size is to be maximized , the first alternatives shown to the user are those with the widest doors , perhaps at the expense of payload or car size .the main issue with liftcreate work - flow is that even simple versions of elevators consists of a large number of components , including car frame , car , doors ( car and landing doors ) , emergency brakes , pistons or cables , motors and control logic . in order to explore the space of potential designs, components can not be solely available as drawing elements , like in classical cad solutions , but they must be handled as first class data inside liftcreate logic .this aspect required us to organize the taxonomy related to different kinds of elevators and , for each elevator kind , to structure the components in a part - whole hierarchy . in figure[ fig : elev_onto ] we show a fragment of the taxonomy for elevators and an example of part - whole structure for a specific elevator kind .in particular , in figure [ fig : elev_onto ] ( top ) , we see that liftcreate classifies * elevator * individuals in two main subclasses corresponding to hydraulic - based ( * hydraulicelevator * ) and rope - based ( * ropeelevator * ) designs. both subclasses feature additional partitions to handle specific design requirements , e.g. , rope elevators can be provided with a reduction gearbox or not , and the drive can be direct of reeved . for one leaf class of the taxonomy , namely * onepistondirecthydraulicelevator* , in figure [ fig : elev_onto ] ( bottom ) we show the detailed part - whole diagram , from which we learn that , e.g. , the only peculiar aspects of such subclass is to have only one * piston * , whereas the remaining components are common to * hydraulicelevator * or * elevator*. also we can see that the car frame is specific of hydraulic elevators ( * carframehydra * ) and it is comprised of several parts , including * carrails * , * buffer * and * ropes*. the relationships encoded in such part - whole hierarchy are instrumental to liftcreate when it comes to handle drawing , storage and retrieval of designs , but also to reason about the various trade - offs of a design when searching in the space of potential solutions .considering the experiences herein outlined , we summarize some lessons learned in applying ontologies for systems engineering .first and foremost , while ontologies provide an effective tool for conceptualizing scenarios as diverse as those considered , some ontology - based tools , e.g. , dl reasoners , are untenable unless small - to - medium scale systems are considered . in the case of e414 ontology reasoning with an expressive ontology required us to implement strategies to `` forget '' data to avoid cluttering the reasoner . in the ils ontology ,where sparql queries for kpis are the only reasoning requested and the usage of owl 2 ql profile banned expressive but hard - to - compute constructs , scaling still requires discarding data using a recency approach . on the other hand , in disegno and liftcreate, ontologies merely provide means for conceptualizing data and , as such , flexibility is gained without sacrificing performances .the second take - home message is that sublanguages of owl 2 are adequate for most modeling purposes . with the only exception of e414 ontology , the ones herein considered fit owl 2 ql constraints which allowed us to combine in a natural way subclassing ( `` is - a '' relationships ) with other kind of object properties ( including `` has - a '' ) .however , the fact that owl 2 ql ontologies can be compiled to relational databases as in the case of disegno or handled trough an object - persistency module as in the case of liftcreate makes their use transparent to other system components .third , and final point , with the exception of ils monitoring , none of our applications required the integration of different data sources which is indeed one of the main tasks which ontologies are advocated for .nevertheless , our experience witnesses that even in single - source data modeling , ontologies provide an excellent mean to bridge the gap between domain experts and computer software designers .cristina de ambrosi , cristiano ghersi , and armando tacchella .an ontology - based condition analyzer for fault classification on railway vehicles . in _22nd int.l conference on industrial , engineering and other applications of applied intelligent systems , iea / aie 2009 , tainan , taiwan , june 24 - 27 , 2009 .proceedings _ , pages 449458 , 2009 .giuseppe cicala , marco de luca , marco oreggia , and armando tacchella . a multi - formalism framework to generate diagnostic decision support systems . in _30th european conference on modelling and simulation , ecms 2016 , regensburg , germany , may 31 - june 3 , 2016 , proceedings ._ , pages 628634 , 2016 .d. calvanese , g. de giacomo , d. lembo , m. lenzerini , and r. rosati . .in _ proceedings of the national conference on artificial intelligence _ , volume 20 , page 602 .menlo park , ca ; cambridge , ma ; london ; aaai press ; mit press ; 1999 , 2005 .robin t. bye , ottar l. osen , birger skogeng pedersen , ibrahim a. hameed , and hans georg schaathun . a software framework for intelligent computer - automated product design . in _30th european conference on modelling and simulation , ecms 2016 , regensburg , germany , may 31 - june 3 , 2016 , proceedings ._ , pages 534543 , 2016 ."," in recent years ontologies enjoyed a growing popularity outside specialized ai communities . system engineering is no exception to this trend , with ontologies being proposed as a basis for several tasks in complex industrial implements , including system design , monitoring and diagnosis . in this paper , we consider four different contributions to system engineering wherein ontologies are instrumental to provide enhancements over traditional ad - hoc techniques . for each application , we briefly report the methodologies , the tools and the results obtained with the goal to provide an assessment of merits and limits of ontologies in such domains . "
"cellular automata ( ca ) models for solar flares are successful in explaining solar flare statistics ( peak flux , total flux , and duration distributions ; lu & hamilton 1991 ( hereafter lh91 ) ; lu et al .1993 ; vlahos et al .1995 ; georgoulis & vlahos 1996 , 1998 ; galsgaard 1996 ) .they simplify strongly the details of the involved physical processes , and achieve in this way to model large volumes with complex field topologies and a large number of events .on the other hand , mhd simulations give insight into the details of the local processes , they are limited , however , to modeling relatively small fractions of active regions , due to the lack of computing power , yielding thus poor statistics and difficulties in comparing results to observations ( e.g.  mikic et al . 1989 ; strauss 1993 ; longcope & sudan 1994 ; einaudi et al . 1996 ; galsgaard & nordlund 1996 ; hendrix & van hoven 1996 ; dmitruk & gomez 1998 ; galtier & pouquet 1998 ; georgoulis et al .1998 ; karpen et al .1998 ; einaudi & velli 1999 ) .the _ global _mhd flare models are still in the state of rather qualitative flare scenarios .the mhd and the ca approach to solar flares seem to have very little in common : the former are a set of partial differential equations , based on fluid - theory and maxwell s equations , whereas the latter are a set of abstract evolution rules , based ( in the case of solar flares ) on the analogy to critical phenomena in ( theoretical ) sand - piles .the scope of this paper is to bridge the gap in - between these two approaches : the solar flare ca models are re - interpreted and extended so as ( i ) to make these models completely compatible with mhd and with maxwell s equations , and so that ( ii ) all relevant mhd variables are made available ( e.g. the current and the electric field , which so far were not available in ca models ) .  in an earlier paper ( isliker et al .1998 ) , we have analyzed the existing solar flare ca models for their soundness with mhd .we asked the question whether the fields in these ca models and the evolution rules can be interpreted in terms of mhd .it turned out that these models can indeed be interpreted as a particular way of implementing numerically the mhd equations .this fact is not trivial , since these models had been derived in quite close analogy to the sand - pile ca model of bak et al .( 1987 and 1988 ) , with vague association of the model s variables with physical quantities .for instance , some authors ( lu et al .1993 ) explicitly discuss the question whether their basic grid variable is the magnetic field or not , without reaching to a definite conclusion .isliker et al .( 1998 ) brought forth not only how the existing ca models are related to mhd and what simplifications are hidden , but also where they differ from or even violate the laws of mhd and maxwell s equation .important is the fact that though the existing ca models can be considered as a strongly simplified numerical solution of the ( simplified ) mhd equations , _ they do not represent the discretized mhd equations _ : the time - step and the spacing between two grid sites are not small ( in a physical sense ) , but finite ; they are a typical temporal and spatial scale of the diffusive processes involved ( see isliker et al .1998 ) .  from the point of view of mhd ,the main short - comings of the existing ca models are ( isliker et al .1998 ) : ( 1 ) there is no control over consistency with maxwell s equations . interpreting , for instance , the vector - field in the ca models as the magnetic field leads to the problem that the gradient of the field ( ) can not be controlled .( 2 ) secondary quantities , such as currents , are not available , and they can not be introduced in the straightforward way by replacing differential expressions by difference - expressions , since , as mentioned , the grid - size must be considered finite ( see also app .this lack of knowing how to calculate derivatives made it also useless to interpret the primary vector - field in the ca models as the vector potential ( to avoid the -problem ) , since could not be derived .the physical interpretation of these ca models remained so far problematic .there are two basically different ways of developing ca models for flares further : ( i ) either one considers ca models _ per se _ , tries to change the existing models further or invent new ones , with the only aim of adjusting them to reproduce still better the observations , i.e. one makes them a tool the results of which explain and maybe predict observed properties of flares . in this approach, one has not to care about possible inconsistencies with mhd or even maxwell s equations , the various components of the model are purely instrumentalistic .( ii ) on the other hand , one may care about the physical identification and interpretation of the various components of the model , not just of its results , and one may want the ca model to become consistent with the other approach to solar flares , namely mhd . in the approach ( ii ) , some of the freedom one has in constructing ca models will possibly be reduced , since there are more boundary conditions to be fulfilled in the construction of the model : the observations must be reproduced and consistency with mhd has to be reached .( trials to construct new ca models which are based on mhd and not on the sand - pile analogy were recently made by einaudi & velli ( 1999 ) , macpherson & mackinnon ( 1999 ) , longcope and noonan ( 2000 ) , and isliker et al .( 2000a ) . )our aim is in - between these two alternatives : we construct a set - up which can be superimposed onto each classical solar flare ca model , and which makes the latter interpretable in a mhd - consistent way ( by _ classical _ ca models we mean the models of lh91 , lu et al .1993 , vlahos et al .1995 , georgoulis & vlahos 1996 , 1998 , galsgaard 1996 , and their modifications , which are based on the sand - pile analogy ) .the set - up thus specifies the physical interpretation of the grid - variables and allows the derivation of quantities such as currents etc .it does not interfere with the dynamics of the ca ( unless wished ) : loading , redistributing ( bursting ) , and the appearance of avalanches and self - organized criticality ( soc ) , if the latter are implied by the evolution rules , remain unchanged .the result is therefore still a ca model , with all the advantages of ca , namely that they are fast , that they model large spatial regions ( and large events ) , and therewith that they yield good statistics .since the set - up introduces all the relevant physical variables into the context of the ca models , it automatically leads to a better physical understanding of the ca models .it reveals which relevant plasma processes and in what form are actually implemented , and what the global flare scenario is the ca models imply .all this was more or less hidden so far in the abstract evolution rules .it leads also to the possibility to change the ca models ( the rules ) at the guide - line of mhd , if this should become desirable .not least , the set - up opens a way for further comparison of the ca models to observations .  in sec . 2, we introduce our set - up . applying it to several ca models ( sec .3 ) , we will demonstrate the usefulness and some of the benefits such extended models ( i.e. classical models extended with our set - up ) provide over the classical ca models , and we will reveal basic physical features of the ca models .the potential of the extended models to explain more observational facts than the classical ca models is , among others , outlined in the conclusions ( sec . 4 ) .the set - up we propose can be superimposed onto solar flare ca models which use a 3d grid and a basic 3d vector grid - variable , say .the corresponding set of evolution rules is not changed .( with a few modifications , the set - up can also be superimposed onto ca models which use a scalar field in a planar grid , which our set - up necessarily interprets as slab geometry , as will become clear later . )we introduce our model on the example of the solar flare ca model of lh91 , which we summarize here in order to make the subsequent presentation more concrete :   in the lh91 model , to each grid - site of a 3d cubic grid a 3d vector is assigned .initially , is set to , everywhere .the system is then loaded with the repeated dropping of increments at randomly chosen sites ( one per time - step ) where has all its components as random numbers uniformly distributed in  ] and , the power - law index , a free parameter .simulations were performed for and .interested in global features implied by the ca model , our concern here is the structure of the magnetic field .it turns out that the magnetic field exhibits still a large scale organization , which is very similar to the one of the -field of the ( extended with our set - up ) lh91 model ( fig .1(b ) ) : for , the respective plots are visually indiscernible , and for the overall shape is still roughly the same , it merely seems slightly more distorted .thus , though the statistical results depend on , the strength and variability of the loading , the structure of the magnetic field remains approximately the same as in the case of the extended model of lh91 .large - scale organization ( in the characteristic form of fig .1 ) must consequently be considered as an inherent property of soc state , through the mechanism explained in sec .3.1.1 .vlahos et al . (1995 ) introduced anisotropic bursts for solar flare ca models , which lead only to small events , but yield a steep distribution at small energies , predicting thus a significant over - abundance of small events with a significant contribution to coronal heating .we have first to generalize the anisotropic evolution rules , which are again for a scalar primary field , to the case of a primary vector field .a natural generalization would be to apply the anisotropic rules to the absolute magnitude of , but it turns out that this causes the algorithm to get trapped in infinite loops ( two neighbouring grid - sites trigger each other mutually for ever ) .the same holds if we apply the anisotropic rules to the absolute magnitudes of the three components of independently .we finally applied the anisotropic rules to the three components of directly , not using absolute magnitudes , as also vlahos et al .( 1995 ) did not use absolute magnitudes , and this turned out to lead to a stationary asymptotic state : the anisotropic stress in the -component is thus defined as where stands for one of the six nearest neighbours .the instability criterion is and the redistribution rules become for the central point and for those nearest neighbours which fulfill the instability criterion ( eq . 15 ) , where the primed sum is over those neighbours for which eq .( 15 ) holds .the rules for are completely analogous ( so that actually there are 18 possibilities to exceed the threshold ( eq .15 ) at a given site ) .the released energy is assumed to amount to   we performed a run where only the anisotropic burst - rules were applied , in order to isolate their effect , although the anisotropic burst - rules are used always together with the isotropic ones by vlahos et al .( 1995 ) , since alone they can not explain the complete energy distributions of flares . in fig . 5 ,the magnitude of the magnetic field at a cut through the grid is shown ( fixed ) , for an arbitrary time ( in the loading phase ) during the asymptotic stationary state of the model . clearly , there is no overall large scale structure anymore , except that the magnetic field along the boundaries is increased .the magnetic field topology is thus nearer to the concept of a random , relatively unstructured magnetic field than the magnetic field topology yielded by the isotropic models in soc state .the anisotropic burst rules do not yield large - scale structures , as they are , when used alone , also not able to lead the system to soc state : this is obvious from the energy distributions they yield , which are much smaller in extent than the ones given by the isotropic rules ( see vlahos et al .1995 ) , and confirmed by the result of lu et al .( 1993 ) that isotropy of the redistribution rules at least on the average is a prerequisite to reach soc state , at all .the anisotropic bursts occur independently in all directions and are in this way not able to organize the field in a neighbourhood systematically , and , as a consequence , also not in the entire grid .the inquiry of the relation of the energy release formula eq .( 18 ) , which is different from the isotropic formula ( eq . 6 ) , to mhd based formulae we leave for a future study .we just note that the distributions the anisotropic model in our vector - field version yields are at lower energies , smaller in extent , and steeper than the ones of the isotropic models .we have introduced a new set - up for classical solar flare ca models which yields , among others , consistency with maxwell s equations ( e.g. divergence - free magnetic field ) , and availability of secondary variables such as currents and electric fields in accordance with mhd .both are new for solar flare ca models .the set - up specifies the so far open physical interpretation of the ca models .this specification is to some extent unavoidably arbitrary , and it would definitely be interesting to see what alternative interpretations would yield if they can be derived consistently .we can claim , however , that the interpretation we chose is reasonable , it is well - behaved in the sense that the derivatives of analytically prescribed vector - potentials are reproduced and that the abstract stress - measure of the ca models is related to the current , due to general properties of spline interpolation .the central problem which was to solve is how to calculate derivatives in a ca model , i.e. how to continue the primary grid - variable in - between the grid sites , since the notion of derivatives is alien in the context of ca models quite in general .  in this article , our main aim withthe introduced set - up was to demonstrate that the set - up truly extends the classical ca models and makes them richer in the sense that they contain much more information , now .the main features we revealed about the ca models , extended with our set - up , are :  * 1 .large - scale organization of the vector - potential and the magnetic field : * the field topology during soc state is bound to characteristic large - scale structures which span the whole grid , very pronounced for the primary grid variable , the vector - potential , but also for the magnetic field .bursts and flares are just slight disturbances propagating over the large - scale structures , which are always maintained , also in the largest events .the magnitude of the current , as a second order derivative of the primary field , does not show any obvious large - scale structure anymore , it reflects more or less only the random fluctuations of the large - scale organized magnetic field .it is worthwhile noting that the large - scale structure of the primary grid - variable is not an artificial result of our set - up , but a natural consequence of the soc state in which the system finds itself .the appearance of large - scale structures for the primary grid variable was shown here for the first time .it may have been known to different authors , but it never has explicitly been shown : soc models for flares are derived in analogy to sand - pile dynamics , and the paradigm of a pile reappears in the field topologies of the solar flare ca models .  * 2 .increased current at unstable grid - sites : * unstable sites are characterized by an enhanced current , which is reduced after a burst has taken place , as a result of which the current at a grid - site in the neighbourhood may be increased .  *availability of the electric field : * the electric field is approximated with the resistive part of ohm s law in its simple form , which can in general be expected to be a good approximation in coronal applications and where the interest is in current - dissipation events , e.g. in the case of solar flares .energy release in terms of ohmic dissipation : * we replaced the some - what _ ad hoc _ formula in the ca models to estimate the energy released in a burst with the expression for ohmic dissipation in terms of the current .the distributions yielded in this way are very similar to the ones based on the ad hoc formula , so that the results of the ca models remain basically unchanged .  *ca as models for current dissipations : * as a consequence of point 2 and 4 in this list , and of the fact that there is an approximate linear relation between the current and the stress measure of the ca , we can conclude that the _ extended _ ca models can be considered as models for energy release through current dissipation .our set - up is to be contrasted to the recently suggested mhd - derived ( not based on the sand - pile analogy ) ca models of einaudi & velli ( 1999 ) , macpherson & mackinnon ( 1999 ) , longcope and noonan ( 2000 ) , and isliker et al .( 2000a ) .they all suggest new evolution rules , derived from mhd , and all in different ways ( they actually focus on different processes , namely the microscopic , macroscopic , and mesoscopic physics , respectively , in active regions ) .our set - up , on the other hand , uses existing ca models , does not interfere ( if not wished ) with their evolution rules , does also not change their main results , as shown , but reinterprets them , extends them essentially , and makes them compatible with mhd.  the set - up we introduced allows different future applications and posing questions which could not be asked so far in the frame of ca models . in preparationis a study ( isliker et al .2000b ) to reveal in detail what physical flare scenario the extended ca models imply .we will address the questions : ( 1 ) how to interpret the small scale processes of the models ( loading and bursting ) in terms of mhd ; ( 2 ) what the _ global _ flare - scenario implied by the models is ; ( 3 ) whether the global magnetic field topology of the models can be considered to represent observed magnetic topologies in active regions ; ( 4 ) what spatio - temporal evolution of the electric field during flares is yielded by the models .  a different future application we plan with ca models extended with our set - up is the introduction of particles into the models , with the aim to study thermal emission , particle acceleration , and non - thermal emission .this will allow a much deeper comparison of the ca models to observations than was possible so far , and this is actually the most important benefit of the set - up we introduced .such comparisons will allow a new judgment of the adequateness or not of classical ca models ( in their current form ) to the problem of solar flares , beyond the three statistical distributions of the primarily released energy .solar flare ca models which include particle acceleration would represent the first global and complete model for solar flares .we thank k. tsiganis and m. georgoulis for many helpful discussions on several issues .we also thank g. einaudi for stimulating discussions on mhd aspects of flares , and the referee a.l .mackinnon for useful comments .the work of h. isliker was partly supported by a grant of the swiss national science foundation ( nf grant nr .8220 - 046504 ) .the temporal evolution of the ca models presented in this article is governed by the following rules :  * initializing * loading * scanning : create a list of the unstable sites ; if there are none , return to loading ( 1 ) * scanning and bursting : redistribute the fields at the unstable sites which are in the list created in the scannings 2 or 4 * scanning : create a list of the unstable sites . if there are any ,go to bursting ( 3 ) , else return to loading ( 1 )  the extra scannings 2 and 4 are needed for causality : if a site becomes unstable through a burst in the neighbourhood , then it should be redistributed in the subsequent scan , and not in the same as the primary unstable site .the same is true for the scanning 4 , since in the next bursting phase ( if any ) only those sites should burst who had become unstable through a burst in their neighbourhood during the foregoing time - step .  as a time - stepis considered one scanning of the grid , point 3 .the released energy per time - step is the sum of all the energy released by bursts in this time - step ( a burst is considered a single redistribution event in 3 ) .we term a flare or avalanche the loop 3,4 , from the occurring of the first burst in 3 until the activity has died out and one returns via the scanning 4 to loading ( 1 ) .the duration of the flare is the number of time - steps it lasted , the total flare energy is the sum of all the energies released in the duration of the flare , and the peak flux or peak energy is the maximum of the energies of all the time - steps of the flare .we mentioned in sec . 2.2 that other possibilities for continuation of the vector - potential besides spline interpolation would be : a ) continuation of with the help of an equation ; b ) other kinds of interpolation , either locally ( in a neighbourhood ) , or globally ( through the whole grid ) .possibility a ) implies that an equation has to be solved in each time - step ( after each loading and after each burst ) , in the worst case numerically , with open boundary condition and the given at the grid - sites .this computational effort might slow down the algorithm of the model considerably ( and bring it near to the computational effort of mhd equation integration ) .besides that , the problem is what equation to use : to make the magnetic field always a potential field ( i.e. using a corresponding equation for the vector - potential ) implies that , from the point of view of mhd , at all times a very well - behaved magnetic field resides in the ca , with no tendency towards instabilities , which makes it difficult to understand why bursts should occur at all , since critical quantities such as currents do not become excited .a better candidate could be expected to be force - freeness , except that , possibly , one may be confronted with incompatibility of the boundary conditions with the vector - potential values given at the grid - sites , i.e. existence - problems for solutions eventually arise .though definitely possibility a ) can not be ruled out on solid grounds , we found it more promising to proceed with possibility b ) , interpolation .a guide - line for choosing a particular interpolation method is the reasonable demand that the interpolation should not introduce wild oscillations in - between grid - sites , for we want to assure that the derivatives at the grid sites , which are very sensitive to such oscillations , are not random values solely due to the interpolation , but that they reflect more or less directly the change of the primary grid - variable from grid - site to grid - site .this calls for interpolating functions which are as little curved as possible .the easiest and fastest way of interpolating would be to perform local interpolations around a point and its nearest neighbours ( e.g. using low - order polynomials or trigonometric functions of different degrees ) .this interpolation leads , however , to ambiguities for the derivatives : the derivatives , say at a point , are not the same , if the used interpolation is centered at , with the ones calculated with an interpolation centered at e.g.  . in this sense ,local interpolation is not self - consistent , the derivatives at a grid - site depend on where the used interpolation is centered .finally , we are left with global interpolation through the whole grid . among the candidatesare , besides more exotic interpolating functions , polynomials of degree equal to the grid size , trigonometric functions ( also in the form of fourier - transforms ) , low - order smooth polynomials ( e.g. splines ) . the first candidate , polynomials of a high degree ( with the number of grid points in one direction ), we reject immediately since it is notorious for its strong oscillations in - between grid - sites , mainly towards the edges of the grid .we tried the second candidate , trigonometric interpolation , in the form of discrete fourier transform . testing this by prescribing analytic functions for and comparing the numerical derivatives with the analytic ones, it turned out that there arise problems with representing structures in as large as the entire grid ( the wave - number spectrum is too limited ) , and with structures as short as roughly the grid - spacing ( different prescribed short structures are taken for the same ) .  trying cubic spline - interpolation, we found that it does not suffer from the problems stated for the other types of interpolation : neither does it introduce wild oscillations , unmotivated by the values at the grid - sites , nor does spline interpolation have problems with describing large or small scale structures ( if a functional form of is prescribed , then the analytic derivatives and the derivatives yielded by the interpolation give very close values , in general ) .moreover , based on results of sec . 3 , app . c , and isliker et al .( 1998 ) , there is another reason why spline - interpolation is particularly adequate to our problem : it relates the quantity ( eq . ( 2 ) ) , which measures the stress at a site in the ca model , closely to , the laplacian of ( see app .the latter is related to the current ( ) , which , from the point of view of mhd , can be considered as a measure of stress in the magnetic field configuration .if this relation would not hold , then the redistribution rules ( eqs . ( 4 ) and ( 5 ) ) of the ca would not be interpretable as the diffusion process revealed by isliker et al .( 1998 ) , and the instability criterion ( eq . 3 ) would not be so closely related to the current ( see sec . 3 and app .we had rejected above ( sec . 1 , sec . 2.2 ) the use of difference expressions to calculate derivatives , stating that differencing is not in the spirit of ca models quite in general , since the nature of ca is truly discrete .we think it worthwhile to make this argument more concrete and to show what problems arise if differencing were used :  isliker et al .( 1998 ) have shown that the classical solar flare ca are not just the discretized form of a differential equations .instead , they describe the time - evolution of a system by rules which express the direct transition from a given initial to a final state which is the asymptotic solution of a simple diffusion equation .the time - step corresponds therewith to the average time needed for smallest scale structures ( structures as large as a neighbourhood ) to diffuse , and the grid - size corresponds to the size of these smallest occurring structures . assuming that the ca models were just discretized differential equations would lead to severe mathematical and physical contradictions and inconsistencies ( continuity for is violated ( with the grid - size ) , and negative diffusivities appear ) .therewith , in order to be consistent with the evolution rules , which assume a finite grid - size , one can not assume for the purpose of differentiating this same grid - size to be approximately infinitesimal .there are several equivalent ways to define numerical derivatives with the use of difference expressions : there are e.g.  the backward difference , and the forward difference . both should give comparable values in a given application , else , in the context of differential equation integration , one would have to make the resolution higher . in the case of ca - models, we find that the two difference expressions yield values which differ substantially from each other : e.g. for an initial loading of the grid with independent random values for the -field , the difference between the backward and the forward difference expression can be as large as the field itself .such an initial condition would of course not make sense in the context of partial differential equations , in the context of ca , however , it is a reasonable starting configuration , and the evolution is unaffected by such an initial loading .moreover , when the ca models we discuss in this article have reached the soc state , then the differences between e.g. the backward- and forward - difference expressions can be as large as 400% .there is no way to reduce this discrepancy , since grid - refinement is principally impossible for ca : the evolution is governed by a set of rules , and making the grid spacing smaller by introducing new grid - points in - between the old ones would actually just mean to make the grid larger , since the evolution rules remain the same , there are no rules for half the grid - spacing .the stress measure of lh91 , , can be related to continuous expressions by representing the values of as taylor - series expansions around , setting the spatial differences to .it turns out that e.g.  and so on for the other two components . _ in general , _ it is therefore _ not _ adequate to consider to be a good 4th order approximation to , since higher order corrections can be large , they depend on the way the vector potential is continued in - between grid - sites . if we had , for instance , chosen global polynomial interpolation instead of spline - interpolation , the higher order terms would not be negligible , above all towards the edges of the grid , since polynomial interpolation is known for introducing fluctuations near the edges of the grid .consequently , would be a bad approximation to . in order to be a good approximation to , interpolation with , for example , 3rd order polynomials would be an optimum choice ( would be an exact approximation to ) .thus , 3rd order polynomials would be the choice for local interpolation , which , however , is not applicable , since it introduces discontinuities in and ( see app .the way out of the dilemma we suggested in this article is the use of cubic splines , which provide global interpolation with 3rd order polynomials , with and continuous , and only third order derivatives are discontinuous ( this is the price of the compromise ) . for splinesthen , eq . ( c.1 ) writes as ,\end{aligned}\ ] ] due to the discontinuities in the 3rd order derivatives ( the superscripts and refer to the right and left derivative , respectively ) . thus , in case where the third order right and left derivatives are not too different , is a good approximation to .isliker , h. , anastasiadis , a. , vlahos , l. , 2000a , a solar flare model in between mhd and cellular automaton , in the fourth astronomical conference of the hellenic astronomical society , eds .seimenis , j. et al ."," a set - up is introduced which can be superimposed onto the existing solar flare cellular automata ( ca ) models , and which specifies the interpretation of the model s variables . it extends the ca models , yielding the magnetic field , the current , and an approximation to the electric field , in a way that is consistent with maxwell s and the mhd equations . applications to several solar flare ca models during their natural state ( self - organized criticality ( soc ) ) show , among others , that ( 1 ) the magnetic field exhibits _ characteristic large - scale organization _ over the entire modeled volume ; ( 2 ) the magnitude of the current seems spatially dis - organized , with no obvious tendency towards large - scale structures or even local organization ; ( 3 ) bursts occur at sites with increased current , and after a burst the current is relaxed ; ( 4 ) by estimating the energy released in individual bursts with the use of the current as ohmic dissipation , it turns out that the power - law distributions of the released energy persist . the ca models , extended with the set - up , can thus be considered as _ models for energy - release through current - dissipation_. the concepts of power - law loading and anisotropic events ( bursts ) in ca models are generalized to 3d vector - field models , and their effect on the magnetic field topology is demonstrated .  = cmr8 = cmcsc10  = ~   # 1#1 "
